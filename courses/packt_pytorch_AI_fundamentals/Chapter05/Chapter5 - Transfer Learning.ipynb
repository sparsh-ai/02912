{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport torch.optim as optim\nfrom torchvision import transforms, datasets, models, utils\nfrom torch.utils.tensorboard import SummaryWriter\nimport time\nimport numpy as np\nfrom torchsummary import summary\nfrom torch.utils.data import DataLoader","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AdaptiveConcatPool2d(nn.Module):\n    def __init__(self, sz=None):\n        super().__init__()\n        sz = sz or (1,1)\n        self.ap = nn.AdaptiveAvgPool2d(sz)\n        self.mp = nn.AdaptiveMaxPool2d(sz)\n    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model():\n    model = models.resnet50(pretrained=True)\n    \n    for param in model.parameters():\n        param.requires_grad = False\n    \n    model.avgpool = AdaptiveConcatPool2d()\n    model.fc = nn.Sequential(\n        nn.Flatten(),\n        nn.BatchNorm1d(4096),\n        nn.Dropout(0.5),\n        nn.Linear(4096, 512),\n        nn.ReLU(),\n        nn.BatchNorm1d(512),\n        nn.Dropout(p=0.5),\n        nn.Linear(512, 2),\n        nn.LogSoftmax(dim=1)\n    )\n    return model","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, device, train_loader, criterion, optimizer, epoch, writer):\n    model.train()\n    \n    total_loss = 0\n    \n    for batch_id, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        \n        optimizer.zero_grad()\n        preds = model(data)\n        loss = criterion(preds, target)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n    writer.add_scalar('Train Loss', total_loss/len(train_loader), epoch)\n    writer.flush()\n    \n    return total_loss/len(train_loader)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test(model, device, test_loader, criterion, epoch, writer):\n    model.eval()\n    \n    total_loss, correct = 0, 0\n    \n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            total_loss += criterion(output, target).item()\n            pred = output.data.max(1)[1]\n            correct += pred.eq(target.data).cpu().sum()\n            \n            misclassified_images(pred, writer, target, data, output, epoch)\n            \n    total_loss /= len(test_loader)\n    accuracy = 100. * correct / len(test_loader.dataset)\n    \n    writer.add_scalar('Test Loss', total_loss, epoch)\n    writer.add_scalar('Accuracy', accuracy, epoch)\n    writer.flush()\n    \n    return total_loss, accuracy","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inv_normalize = transforms.Normalize(\n        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n        std=[1/0.229, 1/0.224, 1/0.255]\n    )","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def misclassified_images(pred, writer, target, data, output, epoch, count=10):\n    misclassified = (pred != target.data)\n    for index, image_tensor in enumerate(data[misclassified][:count]):\n        img_name = '{}->Predict-{}x{}-Actual'.format(\n                epoch,\n                LABEL[pred[misclassified].tolist()[index]],\n                LABEL[target.data[misclassified].tolist()[index]], \n            )\n        \n        writer.add_image(img_name, inv_normalize(image_tensor), epoch)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_transforms = {\n    'train':\n    transforms.Compose([\n        transforms.RandomResizedCrop(size=300, scale=(0.8, 1.1)),\n        transforms.RandomRotation(degrees=10),\n        transforms.ColorJitter(0.4, 0.4, 0.4),\n        transforms.RandomHorizontalFlip(),\n        transforms.CenterCrop(size=256),  # Image net standards\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])  # Imagenet standards\n    ]),\n    'val':\n    transforms.Compose([\n        transforms.Resize(size=300),\n        transforms.CenterCrop(size=256),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'test':\n    transforms.Compose([\n        transforms.Resize(size=300),\n        transforms.CenterCrop(size=256),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datadir = '../input/chest-xray-pneumonia/chest_xray/chest_xray/'\ntraindir = datadir + 'train/'\nvaliddir = datadir + 'test/'\ntestdir = datadir + 'val/'","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_path = \"model.pth\"","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 128","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = {\n    'train':\n    datasets.ImageFolder(root=traindir, transform=image_transforms['train']),\n    'val':\n    datasets.ImageFolder(root=validdir, transform=image_transforms['val']),\n    'test':\n    datasets.ImageFolder(root=testdir, transform=image_transforms['test'])\n}","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataloaders = {\n    'train': DataLoader(data['train'], batch_size=batch_size, shuffle=True),\n    'val': DataLoader(data['val'], batch_size=batch_size, shuffle=True),\n    'test': DataLoader(data['test'], batch_size=batch_size, shuffle=True)\n}","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LABEL = dict((v,k) for k,v in data['train'].class_to_idx.items())","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = get_model().to(device)","execution_count":17,"outputs":[{"output_type":"stream","text":"Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\n100%|██████████| 97.8M/97.8M [00:01<00:00, 74.3MB/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary(model, (3, 256, 256))","execution_count":38,"outputs":[{"output_type":"stream","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 128, 128]           9,408\n       BatchNorm2d-2         [-1, 64, 128, 128]             128\n              ReLU-3         [-1, 64, 128, 128]               0\n         MaxPool2d-4           [-1, 64, 64, 64]               0\n            Conv2d-5           [-1, 64, 64, 64]           4,096\n       BatchNorm2d-6           [-1, 64, 64, 64]             128\n              ReLU-7           [-1, 64, 64, 64]               0\n            Conv2d-8           [-1, 64, 64, 64]          36,864\n       BatchNorm2d-9           [-1, 64, 64, 64]             128\n             ReLU-10           [-1, 64, 64, 64]               0\n           Conv2d-11          [-1, 256, 64, 64]          16,384\n      BatchNorm2d-12          [-1, 256, 64, 64]             512\n           Conv2d-13          [-1, 256, 64, 64]          16,384\n      BatchNorm2d-14          [-1, 256, 64, 64]             512\n             ReLU-15          [-1, 256, 64, 64]               0\n       Bottleneck-16          [-1, 256, 64, 64]               0\n           Conv2d-17           [-1, 64, 64, 64]          16,384\n      BatchNorm2d-18           [-1, 64, 64, 64]             128\n             ReLU-19           [-1, 64, 64, 64]               0\n           Conv2d-20           [-1, 64, 64, 64]          36,864\n      BatchNorm2d-21           [-1, 64, 64, 64]             128\n             ReLU-22           [-1, 64, 64, 64]               0\n           Conv2d-23          [-1, 256, 64, 64]          16,384\n      BatchNorm2d-24          [-1, 256, 64, 64]             512\n             ReLU-25          [-1, 256, 64, 64]               0\n       Bottleneck-26          [-1, 256, 64, 64]               0\n           Conv2d-27           [-1, 64, 64, 64]          16,384\n      BatchNorm2d-28           [-1, 64, 64, 64]             128\n             ReLU-29           [-1, 64, 64, 64]               0\n           Conv2d-30           [-1, 64, 64, 64]          36,864\n      BatchNorm2d-31           [-1, 64, 64, 64]             128\n             ReLU-32           [-1, 64, 64, 64]               0\n           Conv2d-33          [-1, 256, 64, 64]          16,384\n      BatchNorm2d-34          [-1, 256, 64, 64]             512\n             ReLU-35          [-1, 256, 64, 64]               0\n       Bottleneck-36          [-1, 256, 64, 64]               0\n           Conv2d-37          [-1, 128, 64, 64]          32,768\n      BatchNorm2d-38          [-1, 128, 64, 64]             256\n             ReLU-39          [-1, 128, 64, 64]               0\n           Conv2d-40          [-1, 128, 32, 32]         147,456\n      BatchNorm2d-41          [-1, 128, 32, 32]             256\n             ReLU-42          [-1, 128, 32, 32]               0\n           Conv2d-43          [-1, 512, 32, 32]          65,536\n      BatchNorm2d-44          [-1, 512, 32, 32]           1,024\n           Conv2d-45          [-1, 512, 32, 32]         131,072\n      BatchNorm2d-46          [-1, 512, 32, 32]           1,024\n             ReLU-47          [-1, 512, 32, 32]               0\n       Bottleneck-48          [-1, 512, 32, 32]               0\n           Conv2d-49          [-1, 128, 32, 32]          65,536\n      BatchNorm2d-50          [-1, 128, 32, 32]             256\n             ReLU-51          [-1, 128, 32, 32]               0\n           Conv2d-52          [-1, 128, 32, 32]         147,456\n      BatchNorm2d-53          [-1, 128, 32, 32]             256\n             ReLU-54          [-1, 128, 32, 32]               0\n           Conv2d-55          [-1, 512, 32, 32]          65,536\n      BatchNorm2d-56          [-1, 512, 32, 32]           1,024\n             ReLU-57          [-1, 512, 32, 32]               0\n       Bottleneck-58          [-1, 512, 32, 32]               0\n           Conv2d-59          [-1, 128, 32, 32]          65,536\n      BatchNorm2d-60          [-1, 128, 32, 32]             256\n             ReLU-61          [-1, 128, 32, 32]               0\n           Conv2d-62          [-1, 128, 32, 32]         147,456\n      BatchNorm2d-63          [-1, 128, 32, 32]             256\n             ReLU-64          [-1, 128, 32, 32]               0\n           Conv2d-65          [-1, 512, 32, 32]          65,536\n      BatchNorm2d-66          [-1, 512, 32, 32]           1,024\n             ReLU-67          [-1, 512, 32, 32]               0\n       Bottleneck-68          [-1, 512, 32, 32]               0\n           Conv2d-69          [-1, 128, 32, 32]          65,536\n      BatchNorm2d-70          [-1, 128, 32, 32]             256\n             ReLU-71          [-1, 128, 32, 32]               0\n           Conv2d-72          [-1, 128, 32, 32]         147,456\n      BatchNorm2d-73          [-1, 128, 32, 32]             256\n             ReLU-74          [-1, 128, 32, 32]               0\n           Conv2d-75          [-1, 512, 32, 32]          65,536\n      BatchNorm2d-76          [-1, 512, 32, 32]           1,024\n             ReLU-77          [-1, 512, 32, 32]               0\n       Bottleneck-78          [-1, 512, 32, 32]               0\n           Conv2d-79          [-1, 256, 32, 32]         131,072\n      BatchNorm2d-80          [-1, 256, 32, 32]             512\n             ReLU-81          [-1, 256, 32, 32]               0\n           Conv2d-82          [-1, 256, 16, 16]         589,824\n      BatchNorm2d-83          [-1, 256, 16, 16]             512\n             ReLU-84          [-1, 256, 16, 16]               0\n           Conv2d-85         [-1, 1024, 16, 16]         262,144\n      BatchNorm2d-86         [-1, 1024, 16, 16]           2,048\n           Conv2d-87         [-1, 1024, 16, 16]         524,288\n      BatchNorm2d-88         [-1, 1024, 16, 16]           2,048\n             ReLU-89         [-1, 1024, 16, 16]               0\n       Bottleneck-90         [-1, 1024, 16, 16]               0\n           Conv2d-91          [-1, 256, 16, 16]         262,144\n      BatchNorm2d-92          [-1, 256, 16, 16]             512\n             ReLU-93          [-1, 256, 16, 16]               0\n           Conv2d-94          [-1, 256, 16, 16]         589,824\n      BatchNorm2d-95          [-1, 256, 16, 16]             512\n             ReLU-96          [-1, 256, 16, 16]               0\n           Conv2d-97         [-1, 1024, 16, 16]         262,144\n      BatchNorm2d-98         [-1, 1024, 16, 16]           2,048\n             ReLU-99         [-1, 1024, 16, 16]               0\n      Bottleneck-100         [-1, 1024, 16, 16]               0\n          Conv2d-101          [-1, 256, 16, 16]         262,144\n     BatchNorm2d-102          [-1, 256, 16, 16]             512\n            ReLU-103          [-1, 256, 16, 16]               0\n          Conv2d-104          [-1, 256, 16, 16]         589,824\n     BatchNorm2d-105          [-1, 256, 16, 16]             512\n            ReLU-106          [-1, 256, 16, 16]               0\n          Conv2d-107         [-1, 1024, 16, 16]         262,144\n     BatchNorm2d-108         [-1, 1024, 16, 16]           2,048\n            ReLU-109         [-1, 1024, 16, 16]               0\n      Bottleneck-110         [-1, 1024, 16, 16]               0\n          Conv2d-111          [-1, 256, 16, 16]         262,144\n     BatchNorm2d-112          [-1, 256, 16, 16]             512\n            ReLU-113          [-1, 256, 16, 16]               0\n          Conv2d-114          [-1, 256, 16, 16]         589,824\n     BatchNorm2d-115          [-1, 256, 16, 16]             512\n            ReLU-116          [-1, 256, 16, 16]               0\n          Conv2d-117         [-1, 1024, 16, 16]         262,144\n     BatchNorm2d-118         [-1, 1024, 16, 16]           2,048\n            ReLU-119         [-1, 1024, 16, 16]               0\n      Bottleneck-120         [-1, 1024, 16, 16]               0\n          Conv2d-121          [-1, 256, 16, 16]         262,144\n     BatchNorm2d-122          [-1, 256, 16, 16]             512\n            ReLU-123          [-1, 256, 16, 16]               0\n          Conv2d-124          [-1, 256, 16, 16]         589,824\n     BatchNorm2d-125          [-1, 256, 16, 16]             512\n            ReLU-126          [-1, 256, 16, 16]               0\n          Conv2d-127         [-1, 1024, 16, 16]         262,144\n     BatchNorm2d-128         [-1, 1024, 16, 16]           2,048\n            ReLU-129         [-1, 1024, 16, 16]               0\n      Bottleneck-130         [-1, 1024, 16, 16]               0\n          Conv2d-131          [-1, 256, 16, 16]         262,144\n     BatchNorm2d-132          [-1, 256, 16, 16]             512\n            ReLU-133          [-1, 256, 16, 16]               0\n          Conv2d-134          [-1, 256, 16, 16]         589,824\n     BatchNorm2d-135          [-1, 256, 16, 16]             512\n            ReLU-136          [-1, 256, 16, 16]               0\n          Conv2d-137         [-1, 1024, 16, 16]         262,144\n     BatchNorm2d-138         [-1, 1024, 16, 16]           2,048\n            ReLU-139         [-1, 1024, 16, 16]               0\n      Bottleneck-140         [-1, 1024, 16, 16]               0\n          Conv2d-141          [-1, 512, 16, 16]         524,288\n     BatchNorm2d-142          [-1, 512, 16, 16]           1,024\n            ReLU-143          [-1, 512, 16, 16]               0\n          Conv2d-144            [-1, 512, 8, 8]       2,359,296\n     BatchNorm2d-145            [-1, 512, 8, 8]           1,024\n            ReLU-146            [-1, 512, 8, 8]               0\n          Conv2d-147           [-1, 2048, 8, 8]       1,048,576\n     BatchNorm2d-148           [-1, 2048, 8, 8]           4,096\n          Conv2d-149           [-1, 2048, 8, 8]       2,097,152\n     BatchNorm2d-150           [-1, 2048, 8, 8]           4,096\n            ReLU-151           [-1, 2048, 8, 8]               0\n      Bottleneck-152           [-1, 2048, 8, 8]               0\n          Conv2d-153            [-1, 512, 8, 8]       1,048,576\n     BatchNorm2d-154            [-1, 512, 8, 8]           1,024\n            ReLU-155            [-1, 512, 8, 8]               0\n          Conv2d-156            [-1, 512, 8, 8]       2,359,296\n     BatchNorm2d-157            [-1, 512, 8, 8]           1,024\n            ReLU-158            [-1, 512, 8, 8]               0\n          Conv2d-159           [-1, 2048, 8, 8]       1,048,576\n     BatchNorm2d-160           [-1, 2048, 8, 8]           4,096\n            ReLU-161           [-1, 2048, 8, 8]               0\n      Bottleneck-162           [-1, 2048, 8, 8]               0\n          Conv2d-163            [-1, 512, 8, 8]       1,048,576\n     BatchNorm2d-164            [-1, 512, 8, 8]           1,024\n            ReLU-165            [-1, 512, 8, 8]               0\n          Conv2d-166            [-1, 512, 8, 8]       2,359,296\n     BatchNorm2d-167            [-1, 512, 8, 8]           1,024\n            ReLU-168            [-1, 512, 8, 8]               0\n          Conv2d-169           [-1, 2048, 8, 8]       1,048,576\n     BatchNorm2d-170           [-1, 2048, 8, 8]           4,096\n            ReLU-171           [-1, 2048, 8, 8]               0\n      Bottleneck-172           [-1, 2048, 8, 8]               0\nAdaptiveMaxPool2d-173           [-1, 2048, 1, 1]               0\nAdaptiveAvgPool2d-174           [-1, 2048, 1, 1]               0\nAdaptiveConcatPool2d-175           [-1, 4096, 1, 1]               0\n         Flatten-176                 [-1, 4096]               0\n     BatchNorm1d-177                 [-1, 4096]           8,192\n         Dropout-178                 [-1, 4096]               0\n          Linear-179                  [-1, 512]       2,097,664\n            ReLU-180                  [-1, 512]               0\n     BatchNorm1d-181                  [-1, 512]           1,024\n         Dropout-182                  [-1, 512]               0\n          Linear-183                    [-1, 2]           1,026\n      LogSoftmax-184                    [-1, 2]               0\n================================================================\nTotal params: 25,615,938\nTrainable params: 25,615,938\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.75\nForward/backward pass size (MB): 374.42\nParams size (MB): 97.72\nEstimated Total Size (MB): 472.89\n----------------------------------------------------------------\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.NLLLoss()","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = optim.Adam(model.parameters())","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH_to_log_dir = 'logdir/'","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tb_writer():\n    timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n    writer = SummaryWriter(PATH_to_log_dir + timestr)\n    return writer","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"writer = tb_writer()\ndataiter = iter(dataloaders['train'])\nimages, labels = dataiter.next()\ngrid = utils.make_grid([inv_normalize(image) for image in images[:32]])\nwriter.add_image('X-Ray grid', grid, 0)\nwriter.flush()","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_epochs(model, device, dataloaders, criterion, optimizer,epochs, writer):\n    print('{0:>20} | {1:>20} | {2:>20} | {3:>20}  |'.format('Epoch','Training Loss','Test Loss', 'Accuracy'))\n    best_score = np.Inf\n    for epoch in epochs:\n        train_loss = train(model, device, dataloaders['train'], criterion, optimizer, epoch, writer)\n        test_loss, accuracy = test(model, device, dataloaders['val'], criterion, epoch, writer)\n        if test_loss < best_score:\n            best_score = test_loss\n            torch.save(model.state_dict(), model_path)\n        print('{0:>20} | {1:>20} | {2:>20} | {3:>20.2f}% |'.format(epoch,train_loss,test_loss, accuracy))\n        writer.flush()","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_epochs(model, device, dataloaders, criterion, optimizer, range(0,10), writer)","execution_count":24,"outputs":[{"output_type":"stream","text":"               Epoch |        Training Loss |            Test Loss |             Accuracy  |\n                   0 |  0.36176079947774004 |  0.49701107740402223 |                79.17% |\n                   1 |   0.2337999732756033 |  0.38707311153411866 |                84.78% |\n                   2 |  0.20143801955188192 |    0.370448511838913 |                85.26% |\n                   3 |   0.1900221660006337 |   0.3626258075237274 |                86.86% |\n                   4 |   0.1769030857376936 |    0.409450900554657 |                85.90% |\n                   5 |   0.1636709040257989 |  0.36908947825431826 |                86.86% |\n                   6 |  0.16074819299505977 |   0.4182650983333588 |                84.46% |\n                   7 |  0.16151601621290532 |  0.40160723924636843 |                85.58% |\n                   8 |  0.15197279885774706 |   0.4819407343864441 |                84.13% |\n                   9 |  0.14418317559288768 |   0.4331103503704071 |                84.46% |\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"writer.close()","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_state_dict(torch.load(model_path))","execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def unfreeze(model):\n    for param in model.parameters():\n        param.requires_grad = True","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unfreeze(model)","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=0.01)","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"writer = tb_writer()","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_epochs(model, device, dataloaders, criterion, optimizer, range(9,15), writer)","execution_count":31,"outputs":[{"output_type":"stream","text":"               Epoch |        Training Loss |            Test Loss |             Accuracy  |\n                   9 |  0.15972968554351388 |  0.41342413425445557 |                85.10% |\n                  10 |   0.1224460500042613 |   0.3801746487617493 |                86.54% |\n                  11 |   0.1217333177422605 |  0.37790409922599794 |                87.18% |\n                  12 |  0.11098722713749583 |   0.3712982594966888 |                87.98% |\n                  13 |  0.09877484373566581 |  0.41088773012161256 |                86.70% |\n                  14 |  0.09256085244620718 |   0.3181425631046295 |                89.42% |\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"writer.close()","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}